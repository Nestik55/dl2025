{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "VC7qUe4VHClN"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Optimizer\n",
    "from torch.autograd import Function\n",
    "from typing import List, Optional, Tuple, Union\n",
    "import torch.nn.functional as F\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "-wiwRCesManH"
   },
   "outputs": [],
   "source": [
    "class Lion(Optimizer):\n",
    "  def __init__(self, params, lr=1e-4, betas=(0.9, 0.99), weight_decay=0.0):\n",
    "    defaults = dict(lr=lr, betas=betas, weight_decay=weight_decay)\n",
    "    super().__init__(params, defaults)\n",
    "\n",
    "  @torch.no_grad()\n",
    "  def step(self, closure=None):\n",
    "    loss = None\n",
    "    if closure is not None:\n",
    "      with torch.enable_grad():\n",
    "        loss = closure()\n",
    "\n",
    "    for group in self.param_groups:\n",
    "      for p in group['params']:\n",
    "        if p.grad is None:\n",
    "          continue\n",
    "\n",
    "        p.data.mul_(1 - group['lr'] * group['weight_decay'])\n",
    "\n",
    "        grad = p.grad\n",
    "        state = self.state[p]\n",
    "        if len(state) == 0:\n",
    "          state['exp_avg'] = torch.zeros_like(p)\n",
    "\n",
    "        exp_avg = state['exp_avg']\n",
    "        beta1, beta2 = group['betas']\n",
    "\n",
    "        update = exp_avg * beta1 + grad * (1 - beta1)\n",
    "\n",
    "        p.add_(update.sign_(), alpha=-group['lr'])\n",
    "\n",
    "        exp_avg.mul_(beta2).add_(grad, alpha=1 - beta2)\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vTxX9uENMU0p",
    "outputId": "f32fe3e8-23bc-449b-ea86-a4642f0aaa1b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Maximum difference between outputs: 0.000005\n",
      "Mean difference between outputs: 0.000001\n"
     ]
    }
   ],
   "source": [
    "class SimpleRMSNorm(nn.Module):\n",
    "    def __init__(self, dim, eps=1e-6):\n",
    "        super().__init__()\n",
    "        self.eps = eps\n",
    "        self.weight = nn.Parameter(torch.ones(dim))\n",
    "\n",
    "    def forward(self, x):\n",
    "        rms = torch.sqrt(torch.mean(x ** 2, dim=-1, keepdim=True))\n",
    "\n",
    "        x_norm = x / (rms + self.eps)\n",
    "        return x_norm * self.weight\n",
    "\n",
    "def compare_rmsnorm():\n",
    "    batch_size = 32\n",
    "    seq_len = 128\n",
    "    hidden_dim = 512\n",
    "    x = torch.randn(batch_size, seq_len, hidden_dim)\n",
    "    x = x.to(device)\n",
    "\n",
    "    simple_norm = SimpleRMSNorm(hidden_dim)\n",
    "    builtin_norm = nn.RMSNorm(hidden_dim)\n",
    "\n",
    "    simple_norm = simple_norm.to(device)\n",
    "    builtin_norm = builtin_norm.to(device)\n",
    "\n",
    "    out_simple = simple_norm(x)\n",
    "    out_builtin = builtin_norm(x)\n",
    "\n",
    "    max_diff = torch.max(torch.abs(out_simple - out_builtin))\n",
    "    mean_diff = torch.mean(torch.abs(out_simple - out_builtin))\n",
    "    print(f\"Maximum difference between outputs: {max_diff:.6f}\")\n",
    "    print(f\"Mean difference between outputs: {mean_diff:.6f}\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "compare_rmsnorm()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Kf3dwe_YHSA2",
    "outputId": "4a7612a4-beec-45e3-95fd-c557fec9f200"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Comparing implementations for x=1.0, y=0.7853981633974483:\n",
      "\n",
      "Function values:\n",
      "Custom implementation: 3.4253885746002197\n",
      "PyTorch implementation: 3.4253885746002197\n",
      "Difference: 0.0\n",
      "\n",
      "gradients with respect to x:\n",
      "custom implementation: 2.7182817459106445\n",
      "pytorch implementation: 2.7182817459106445\n",
      "difference: 0.0\n",
      "\n",
      "gradients with respect to y:\n",
      "custom implementation: -0.7071067690849304\n",
      "pytorch implementation: -0.7071067690849304\n",
      "difference: 0.0\n",
      "\n",
      "==================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ExpCosFunction(Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, x, y):\n",
    "        ctx.save_for_backward(x, y)\n",
    "\n",
    "        exp_x = torch.exp(x)\n",
    "        cos_y = torch.cos(y)\n",
    "        return exp_x + cos_y\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        x, y = ctx.saved_tensors\n",
    "\n",
    "        grad_x = grad_output * torch.exp(x)\n",
    "        grad_y = grad_output * -torch.sin(y)\n",
    "        return grad_x, grad_y\n",
    "\n",
    "def compute_function_custom(x, y):\n",
    "\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "\n",
    "    result = ExpCosFunction.apply(x_tensor, y_tensor)\n",
    "\n",
    "    result.backward()\n",
    "\n",
    "    return {\n",
    "        'value': result.item(),\n",
    "        'grad_x': x_tensor.grad.item(),\n",
    "        'grad_y': y_tensor.grad.item()\n",
    "    }\n",
    "\n",
    "def compute_function_torch(x, y):\n",
    "\n",
    "    x_tensor = torch.tensor(x, dtype=torch.float32, requires_grad=True)\n",
    "    y_tensor = torch.tensor(y, dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "    result = torch.exp(x_tensor) + torch.cos(y_tensor)\n",
    "\n",
    "    result.backward()\n",
    "\n",
    "    return {\n",
    "        'value': result.item(),\n",
    "        'grad_x': x_tensor.grad.item(),\n",
    "        'grad_y': y_tensor.grad.item()\n",
    "    }\n",
    "\n",
    "def compare_implementations(x_val, y_val):\n",
    "    custom_result = compute_function_custom(x_val, y_val)\n",
    "    torch_result = compute_function_torch(x_val, y_val)\n",
    "\n",
    "    print(f\"\\nComparing implementations for x={x_val}, y={y_val}:\")\n",
    "    print(\"\\nFunction values:\")\n",
    "    print(f\"Custom implementation: {custom_result['value']}\")\n",
    "    print(f\"PyTorch implementation: {torch_result['value']}\")\n",
    "    print(f\"Difference: {abs(custom_result['value'] - torch_result['value'])}\")\n",
    "\n",
    "    print(\"\\ngradients with respect to x:\")\n",
    "    print(f\"custom implementation: {custom_result['grad_x']}\")\n",
    "    print(f\"pytorch implementation: {torch_result['grad_x']}\")\n",
    "    print(f\"difference: {abs(custom_result['grad_x'] - torch_result['grad_x'])}\")\n",
    "\n",
    "    print(\"\\ngradients with respect to y:\")\n",
    "    print(f\"custom implementation: {custom_result['grad_y']}\")\n",
    "    print(f\"pytorch implementation: {torch_result['grad_y']}\")\n",
    "    print(f\"difference: {abs(custom_result['grad_y'] - torch_result['grad_y'])}\")\n",
    "\n",
    "test_cases = [\n",
    "  (1.0, torch.pi/4),\n",
    "]\n",
    "\n",
    "for x_val, y_val in test_cases:\n",
    "    compare_implementations(x_val, y_val)\n",
    "    print(\"\\n\" + \"=\"*50 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "8katpmL3IYwt"
   },
   "outputs": [],
   "source": [
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        return F.log_softmax(x, dim=1)\n",
    "\n",
    "def train(model, device, train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = F.nll_loss(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Train Epoch: {epoch} [{batch_idx * len(data)}/{len(train_loader.dataset)} '\n",
    "                  f'({100. * batch_idx / len(train_loader):.0f}%)]\\tLoss: {loss.item():.6f}')\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "def test(model, device, test_loader):\n",
    "    model.eval()\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "            test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
    "            pred = output.argmax(dim=1, keepdim=True)\n",
    "            correct += pred.eq(target.view_as(pred)).sum().item()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    accuracy = 100. * correct / len(test_loader.dataset)\n",
    "    print(f'\\nTest set: Average loss: {test_loss:.4f}, '\n",
    "          f'Accuracy: {correct}/{len(test_loader.dataset)} ({accuracy:.2f}%)\\n')\n",
    "    return test_loss, accuracy\n",
    "\n",
    "def plot_metrics(lion_metrics, adam_metrics, save_path='comparison.png'):\n",
    "    epochs = range(1, len(lion_metrics['train_loss']) + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 5))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, lion_metrics['train_loss'], 'b-', label='Lion')\n",
    "    plt.plot(epochs, adam_metrics['train_loss'], 'r-', label='Adam')\n",
    "    plt.title('Training Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    # Plot test accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, lion_metrics['test_acc'], 'b-', label='Lion')\n",
    "    plt.plot(epochs, adam_metrics['test_acc'], 'r-', label='Adam')\n",
    "    plt.title('Test Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy (%)')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(save_path)\n",
    "    plt.close()\n",
    "\n",
    "def train_with_optimizer(optimizer_name, model, device, train_loader, test_loader, epochs):\n",
    "    if optimizer_name == 'Lion':\n",
    "        optimizer = Lion(model.parameters(), lr=1e-4)\n",
    "    else:  # Adam\n",
    "        optimizer = Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        train_loss = train(model, device, train_loader, optimizer, epoch)\n",
    "        test_loss, test_acc = test(model, device, test_loader)\n",
    "\n",
    "        train_losses.append(train_loss)\n",
    "        test_losses.append(test_loss)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "    return {\n",
    "        'train_loss': train_losses,\n",
    "        'test_loss': test_losses,\n",
    "        'test_acc': test_accuracies\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UjdrqmBxMxWT",
    "outputId": "de82ee19-4969-4176-b7bf-d2d2b7f6fda7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9.91M/9.91M [00:00<00:00, 12.7MB/s]\n",
      "100%|██████████| 28.9k/28.9k [00:00<00:00, 340kB/s]\n",
      "100%|██████████| 1.65M/1.65M [00:00<00:00, 3.18MB/s]\n",
      "100%|██████████| 4.54k/4.54k [00:00<00:00, 6.79MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training with Lion optimizer...\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.318191\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.622671\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.303960\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.261303\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.330102\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.176553\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.105868\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.215543\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.056673\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.189472\n",
      "\n",
      "Test set: Average loss: 0.0482, Accuracy: 9850/10000 (98.50%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.019982\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.043420\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.173935\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.096131\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.030287\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.049528\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.155729\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.018898\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.006690\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.149494\n",
      "\n",
      "Test set: Average loss: 0.0371, Accuracy: 9873/10000 (98.73%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.047791\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.101301\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.025432\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.006940\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.107916\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.002629\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.069454\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.027139\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.061594\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.028826\n",
      "\n",
      "Test set: Average loss: 0.0308, Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.024684\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.011822\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.088066\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.018182\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.037983\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.058287\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.213060\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.010902\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.091832\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.008867\n",
      "\n",
      "Test set: Average loss: 0.0272, Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.039436\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.008167\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.039968\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.020935\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.002579\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.010687\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.024859\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.039829\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.086583\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.007100\n",
      "\n",
      "Test set: Average loss: 0.0336, Accuracy: 9898/10000 (98.98%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.004932\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.028321\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.013456\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.000134\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.113298\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.026685\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.004688\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.005880\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.047838\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.007970\n",
      "\n",
      "Test set: Average loss: 0.0302, Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.060267\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.030609\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.001928\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.013758\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.135921\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.117938\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.006734\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.044799\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.014475\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.004066\n",
      "\n",
      "Test set: Average loss: 0.0316, Accuracy: 9906/10000 (99.06%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.006683\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.120819\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.031254\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.063632\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.013071\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.025267\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.002332\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.024958\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.015025\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.030872\n",
      "\n",
      "Test set: Average loss: 0.0311, Accuracy: 9924/10000 (99.24%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.025354\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.000978\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.021955\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.018289\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.002129\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.086072\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.009059\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.001753\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.016731\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.011764\n",
      "\n",
      "Test set: Average loss: 0.0363, Accuracy: 9909/10000 (99.09%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.033941\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.002532\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.017998\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.037192\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.038285\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.006490\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.002448\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.000625\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.016873\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.005244\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9914/10000 (99.14%)\n",
      "\n",
      "\n",
      "Training with Adam optimizer...\n",
      "Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.314376\n",
      "Train Epoch: 1 [6400/60000 (11%)]\tLoss: 0.511811\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 0.438060\n",
      "Train Epoch: 1 [19200/60000 (32%)]\tLoss: 0.474548\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 0.396399\n",
      "Train Epoch: 1 [32000/60000 (53%)]\tLoss: 0.284499\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 0.136086\n",
      "Train Epoch: 1 [44800/60000 (75%)]\tLoss: 0.196722\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 0.106487\n",
      "Train Epoch: 1 [57600/60000 (96%)]\tLoss: 0.176267\n",
      "\n",
      "Test set: Average loss: 0.1067, Accuracy: 9685/10000 (96.85%)\n",
      "\n",
      "Train Epoch: 2 [0/60000 (0%)]\tLoss: 0.145499\n",
      "Train Epoch: 2 [6400/60000 (11%)]\tLoss: 0.246699\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 0.148205\n",
      "Train Epoch: 2 [19200/60000 (32%)]\tLoss: 0.183076\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 0.178403\n",
      "Train Epoch: 2 [32000/60000 (53%)]\tLoss: 0.092162\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 0.139765\n",
      "Train Epoch: 2 [44800/60000 (75%)]\tLoss: 0.174134\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 0.220325\n",
      "Train Epoch: 2 [57600/60000 (96%)]\tLoss: 0.109838\n",
      "\n",
      "Test set: Average loss: 0.0637, Accuracy: 9798/10000 (97.98%)\n",
      "\n",
      "Train Epoch: 3 [0/60000 (0%)]\tLoss: 0.098782\n",
      "Train Epoch: 3 [6400/60000 (11%)]\tLoss: 0.109797\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 0.175737\n",
      "Train Epoch: 3 [19200/60000 (32%)]\tLoss: 0.091489\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.041614\n",
      "Train Epoch: 3 [32000/60000 (53%)]\tLoss: 0.036994\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.093105\n",
      "Train Epoch: 3 [44800/60000 (75%)]\tLoss: 0.049723\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 0.011742\n",
      "Train Epoch: 3 [57600/60000 (96%)]\tLoss: 0.166918\n",
      "\n",
      "Test set: Average loss: 0.0527, Accuracy: 9825/10000 (98.25%)\n",
      "\n",
      "Train Epoch: 4 [0/60000 (0%)]\tLoss: 0.071716\n",
      "Train Epoch: 4 [6400/60000 (11%)]\tLoss: 0.440321\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.075564\n",
      "Train Epoch: 4 [19200/60000 (32%)]\tLoss: 0.025555\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 0.103977\n",
      "Train Epoch: 4 [32000/60000 (53%)]\tLoss: 0.033561\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.105620\n",
      "Train Epoch: 4 [44800/60000 (75%)]\tLoss: 0.055498\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 0.146236\n",
      "Train Epoch: 4 [57600/60000 (96%)]\tLoss: 0.096122\n",
      "\n",
      "Test set: Average loss: 0.0422, Accuracy: 9854/10000 (98.54%)\n",
      "\n",
      "Train Epoch: 5 [0/60000 (0%)]\tLoss: 0.038666\n",
      "Train Epoch: 5 [6400/60000 (11%)]\tLoss: 0.021425\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.070610\n",
      "Train Epoch: 5 [19200/60000 (32%)]\tLoss: 0.042601\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 0.033551\n",
      "Train Epoch: 5 [32000/60000 (53%)]\tLoss: 0.053694\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.103370\n",
      "Train Epoch: 5 [44800/60000 (75%)]\tLoss: 0.099658\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.035299\n",
      "Train Epoch: 5 [57600/60000 (96%)]\tLoss: 0.212492\n",
      "\n",
      "Test set: Average loss: 0.0372, Accuracy: 9870/10000 (98.70%)\n",
      "\n",
      "Train Epoch: 6 [0/60000 (0%)]\tLoss: 0.020904\n",
      "Train Epoch: 6 [6400/60000 (11%)]\tLoss: 0.015837\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.023372\n",
      "Train Epoch: 6 [19200/60000 (32%)]\tLoss: 0.028127\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.031891\n",
      "Train Epoch: 6 [32000/60000 (53%)]\tLoss: 0.024116\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.036724\n",
      "Train Epoch: 6 [44800/60000 (75%)]\tLoss: 0.107921\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.087879\n",
      "Train Epoch: 6 [57600/60000 (96%)]\tLoss: 0.043940\n",
      "\n",
      "Test set: Average loss: 0.0325, Accuracy: 9884/10000 (98.84%)\n",
      "\n",
      "Train Epoch: 7 [0/60000 (0%)]\tLoss: 0.026001\n",
      "Train Epoch: 7 [6400/60000 (11%)]\tLoss: 0.020471\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.038923\n",
      "Train Epoch: 7 [19200/60000 (32%)]\tLoss: 0.073287\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 0.040059\n",
      "Train Epoch: 7 [32000/60000 (53%)]\tLoss: 0.031651\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.031867\n",
      "Train Epoch: 7 [44800/60000 (75%)]\tLoss: 0.028744\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.032156\n",
      "Train Epoch: 7 [57600/60000 (96%)]\tLoss: 0.023188\n",
      "\n",
      "Test set: Average loss: 0.0303, Accuracy: 9897/10000 (98.97%)\n",
      "\n",
      "Train Epoch: 8 [0/60000 (0%)]\tLoss: 0.074503\n",
      "Train Epoch: 8 [6400/60000 (11%)]\tLoss: 0.015434\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 0.065901\n",
      "Train Epoch: 8 [19200/60000 (32%)]\tLoss: 0.072857\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.110853\n",
      "Train Epoch: 8 [32000/60000 (53%)]\tLoss: 0.021883\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 0.006276\n",
      "Train Epoch: 8 [44800/60000 (75%)]\tLoss: 0.084146\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.019769\n",
      "Train Epoch: 8 [57600/60000 (96%)]\tLoss: 0.015151\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9894/10000 (98.94%)\n",
      "\n",
      "Train Epoch: 9 [0/60000 (0%)]\tLoss: 0.018895\n",
      "Train Epoch: 9 [6400/60000 (11%)]\tLoss: 0.021498\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.020492\n",
      "Train Epoch: 9 [19200/60000 (32%)]\tLoss: 0.048738\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 0.029774\n",
      "Train Epoch: 9 [32000/60000 (53%)]\tLoss: 0.039130\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.025750\n",
      "Train Epoch: 9 [44800/60000 (75%)]\tLoss: 0.028901\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.025667\n",
      "Train Epoch: 9 [57600/60000 (96%)]\tLoss: 0.018605\n",
      "\n",
      "Test set: Average loss: 0.0284, Accuracy: 9902/10000 (99.02%)\n",
      "\n",
      "Train Epoch: 10 [0/60000 (0%)]\tLoss: 0.009656\n",
      "Train Epoch: 10 [6400/60000 (11%)]\tLoss: 0.023578\n",
      "Train Epoch: 10 [12800/60000 (21%)]\tLoss: 0.020454\n",
      "Train Epoch: 10 [19200/60000 (32%)]\tLoss: 0.016400\n",
      "Train Epoch: 10 [25600/60000 (43%)]\tLoss: 0.010997\n",
      "Train Epoch: 10 [32000/60000 (53%)]\tLoss: 0.014441\n",
      "Train Epoch: 10 [38400/60000 (64%)]\tLoss: 0.033593\n",
      "Train Epoch: 10 [44800/60000 (75%)]\tLoss: 0.005675\n",
      "Train Epoch: 10 [51200/60000 (85%)]\tLoss: 0.022459\n",
      "Train Epoch: 10 [57600/60000 (96%)]\tLoss: 0.065352\n",
      "\n",
      "Test set: Average loss: 0.0315, Accuracy: 9895/10000 (98.95%)\n",
      "\n",
      "\n",
      "Comparison plot saved as 'comparison.png'\n",
      "\n",
      "Final Results:\n",
      "Lion - Best accuracy: 99.24%\n",
      "Adam - Best accuracy: 99.02%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.1307,), (0.3081,))\n",
    "])\n",
    "\n",
    "train_dataset = datasets.MNIST('data', train=True, download=True, transform=transform)\n",
    "test_dataset = datasets.MNIST('data', train=False, transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000)\n",
    "\n",
    "epochs = 10\n",
    "\n",
    "\n",
    "print(\"\\nTraining with Lion optimizer...\")\n",
    "lion_model = SimpleCNN().to(device)\n",
    "lion_metrics = train_with_optimizer('Lion', lion_model, device, train_loader, test_loader, epochs)\n",
    "\n",
    "\n",
    "print(\"\\nTraining with Adam optimizer...\")\n",
    "adam_model = SimpleCNN().to(device)\n",
    "adam_metrics = train_with_optimizer('Adam', adam_model, device, train_loader, test_loader, epochs)\n",
    "\n",
    "\n",
    "plot_metrics(lion_metrics, adam_metrics)\n",
    "print(\"\\nComparison plot saved as 'comparison.png'\")\n",
    "\n",
    "\n",
    "print(\"\\nFinal Results:\")\n",
    "print(f\"Lion - Best accuracy: {max(lion_metrics['test_acc']):.2f}%\")\n",
    "print(f\"Adam - Best accuracy: {max(adam_metrics['test_acc']):.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
